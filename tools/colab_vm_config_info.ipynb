{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab_vm_config_info.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeC-AbggYh6s",
        "colab_type": "text"
      },
      "source": [
        "## Colab configuration information\n",
        "\n",
        "This notebook is intended simply for dumping status information from a Colab runtime (kernel, VM, whatever)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3SbviHli8Er",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Various installs of packages imported by colab_vm_config_info.ipynb\n",
        "!pip install humanize\n",
        "!pip install gputil\n",
        "!pip install psutil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv-SjRT3KsUT",
        "colab_type": "text"
      },
      "source": [
        "# Python environment stats\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhQ_f7n1K_eK",
        "colab_type": "code",
        "outputId": "608c5215-7c74-41e4-8b11-3127142108a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import platform\n",
        "\n",
        "a_message = \"Python runtime version: \" + platform.python_version() \n",
        "print(a_message)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python runtime version: 3.6.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxbfElYWtEGX",
        "colab_type": "text"
      },
      "source": [
        "# Check VM Uptime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUVolHFGAELj",
        "colab_type": "text"
      },
      "source": [
        "Supposedly these Colab kernel (runtimes, VMs, whatever) are thrown away after a max of 12 hours. In the results of `uptime` is the time-of-day (formatted `HH:MM:SS`) followed by ` up: ` followed by how long the VM has been up (formatted as `XX min`, or after an hour as `H:MM`).\n",
        "\n",
        "Note, the 12 hour lifetime cap is the max but less can happen it seems. Seemingly, [this issue is not well defined by google](https://stackoverflow.com/questions/55050988/can-i-run-a-google-colab-free-edition-script-and-then-shutdown-my-computer).\n",
        "\n",
        "Furthermore, VM max lifetime is different than the constant runtime disconnects. These seem to happen on the order of 60 to 90 minutes. Supposedly keeping a code cell running can keep the connection from disconnect timeouts. Note though that editing text cells does not reset the timeout clock."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw4ETaUArMJK",
        "colab_type": "code",
        "outputId": "ff66a2f4-3e1f-4de2-b736-492b822c9462",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# How long has this VM been up? \n",
        "!uptime"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 08:25:49 up  1:00,  0 users,  load average: 0.12, 0.10, 0.04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xey9UaJVCyqR",
        "colab_type": "text"
      },
      "source": [
        "# Reset VM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvI-zBE8dwud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reset VM\n",
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rim_Xy3h--W",
        "colab_type": "text"
      },
      "source": [
        "# CPU Information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd9OM-gSYdaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How many CPUs does this machine have?\n",
        "!lscpu | grep \"^CPU(s):\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7eY-Lq9dFBy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# More details on the CPU(s):\n",
        "!cat /proc/cpuinfo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhb8bXJIZU9t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RAM\n",
        "!cat /proc/meminfo | head -n3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeSy0EpOigS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RAM info humanized\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "\n",
        "process = psutil.Process(os.getpid())\n",
        "print(\"RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl8OjjKSEp_4",
        "colab_type": "text"
      },
      "source": [
        "# File System\n",
        "\n",
        "Kinda like with AWS Lambda, there are only one or two switches on the VM you can ask for. Memory and CPU are provided as matching packages, not independently configurable.\n",
        "\n",
        "So, depending on what you ask for in terms of compute (CPU, GPU, [TPU](https://colab.research.google.com/notebooks/tpu.ipynb)) you get more or less file system memory [[*](https://stackoverflow.com/a/55890688)]. Note: for all compute options, the OS files initialize to consuming about 25MB of the file system before you are dropped into the kernel. \n",
        "\n",
        "On 2019-05-19, the following tests gave these results:\n",
        "\n",
        "Processor | FS Free GB | FS Total GB \n",
        "--|--|--\n",
        "CPU | 24 | 49\n",
        "GPU | 318 | 359\n",
        "TPU | 26| 49\n",
        "\n",
        "The low FS size for the TPU is probably because for the TPU case, those (the actually TPU boards) are separate machines while for GPUs those  are a part of the machine the notebook is running on. So, for the CPU and the TPU options, Google is probably providing the same VM, ergo the file systems are essentially the same size. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XITlujDERuVd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "85bc78db-addf-40c4-88ca-a894cfb34b8f"
      },
      "source": [
        "# Hardware accelerator none\n",
        "!df -h .\n",
        "#\n",
        "# Result on 2019-05-19:\n",
        "# Filesystem      Size  Used Avail Use% Mounted on\n",
        "# overlay          49G   22G   26G  46% /\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay          49G   23G   24G  49% /\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOxZFgX0Rued",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hardware accelerator GPU\n",
        "!df -h .\n",
        "#\n",
        "# Result on 2019-05-19:\n",
        "#Filesystem      Size  Used Avail Use% Mounted on\n",
        "#overlay         359G   23G  318G   7% /\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On00OiybRuma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hardware accelerator TPU\n",
        "!df -h .\n",
        "#\n",
        "# Result on 2019-05-19:\n",
        "#Filesystem      Size  Used Avail Use% Mounted on\n",
        "#overlay          49G   22G   26G  46% /"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeD7R1g_iFLO",
        "colab_type": "text"
      },
      "source": [
        "# GPU Information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-Oo_YsDbYUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# What GPUs are available for use?\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-tpXTLhbsg7",
        "colab_type": "code",
        "outputId": "753b9924-af79-4e35-f8c5-3150ed87241a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# What GPU is currently config'd for use?\n",
        "\n",
        "# https://colab.research.google.com/notebooks/gpu.ipynb\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\"TensorFlow found NO GPU\")\n",
        "else:\n",
        "  print('TensorFlow found GPU at: {}'.format(device_name))\n",
        "\n",
        "# TODO: Another way?\n",
        "#from tensorflow.python.client import device_lib\n",
        "#device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow found NO GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnpoONR1hBEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Memory in GPU\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "\n",
        "import GPUtil as GPU\n",
        "\n",
        "gpus = GPU.getGPUs()\n",
        "\n",
        "if len(gpus) > 0:\n",
        "  gpu = gpus[0]\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "else:\n",
        "  print(\"No GPU config'd for use.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc4jEhFyZDRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Disk space\n",
        "!df -h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5t8eFMPaJdc",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n",
        "* [Google Colab Free GPU Tutorial](https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d)\n",
        "* [GPU stats code](https://stackoverflow.com/questions/48750199/google-colaboratory-misleading-information-about-its-gpu-only-5-ram-available)\n",
        "* [TensorFlow with GPU](https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=3IEVK-KFxi5Z)"
      ]
    }
  ]
}